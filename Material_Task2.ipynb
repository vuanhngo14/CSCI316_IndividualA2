{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Artificial Neuron Network (ANN) in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover and visualize data file `train.csv`\n",
    "1. Read in the data file using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"../magic04.data\")\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display information of the dataset that is just read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Display important statistic attribute of all features to see whether to drop any not column that has large differences in the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Print out the correlation of the features and the target to find out their relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data\n",
    "#### Visualize the target class values to see if there is any abnormal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dataframe['critical_temp'], edgecolor='maroon', facecolor='orangered')\n",
    "plt.xlabel(\"Temperature (Celsius)\")\n",
    "plt.ylabel(\"Number of temperature label\")\n",
    "plt.title(\"Temperature of for the target class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the number of elements feature to see if there is any abnormal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dataframe['number_of_elements'], edgecolor='maroon', facecolor='orangered')\n",
    "plt.xlabel(\"Numbers of elements\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Features numbers of elements data distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking null value in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a copy of dataset, then split into 2/3 for training and the rest for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe.copy()\n",
    "\n",
    "X = df.drop(columns=['critical_temp'])\n",
    "y = df['critical_temp']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling\n",
    "Since it is a linear regression problem ( which aims to find the best-fitting line that describes the relationship between the independent variables (features) and the dependent variable (target)), scaling all feature and the target class to optimize the algorithm of finding the best fitting line and also help the learning model to be trained faster.\n",
    "1. Using MinMaxScaler()\n",
    "2. fit into the object of this scaler training features dataset\n",
    "3. Using that standard to scale the testing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform the target with same method of fitting the training label to the scaler\n",
    "and scale the testing target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(y_train.values.reshape(-1,1))\n",
    "y_train = scaler.transform(y_train.values.reshape(-1,1))\n",
    "y_train = y_train.reshape(-1)\n",
    "y_train = pd.Series(y_train)\n",
    "y_test = scaler.transform(y_test.values.reshape(-1,1))\n",
    "y_test = y_test.reshape(-1)\n",
    "y_test = pd.Series(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the training data set into two subsets, one for training and one for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X_train[:3000]\n",
    "partial_X_train = X_train[3000:]\n",
    "y_val = y_train[:3000]\n",
    "partial_y_train = y_train[3000:]\n",
    "partial_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function to build a artificial neuron network using Keras API\n",
    "1. `build_model()` function takes in for parameters\n",
    "    * Number of hidden layers : `n_hidden`\n",
    "    * Number of neurons of each layer : `n_neurons`\n",
    "    * Regularization number (L1, L2): `a`, `b`\n",
    "2. Model using the optimizers of `RMSprop` with a `learning_rate` equals 0.0002\n",
    "3. Model using Mean Squared Error as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.applications.densenet import layers\n",
    "from keras import metrics\n",
    "\n",
    "optimizer=keras.optimizers.Adam()\n",
    "def build_model(n_hidden= 3, n_neurons = 32, a = 0.001, b=0.002):\n",
    "    model = keras.Sequential()\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(layers.Dense(n_neurons, activation = 'relu', kernel_regularizer=regularizers.L1L2(l1=a, l2=b)))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(), metrics=[metrics.MeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting a model training data subset**\n",
    "* With 60 iterations and a `batch_size` for dataset of 512\n",
    "* Print out two loss with a validation dataset when fit into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict a temperature for a testing data using the trained model above and print out the results**\n",
    "* Inserse the prediction into original scale to be friendlier in reading the data temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the two loss during fitting into the model to check the efficiency of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = history.history['mean_squared_error']\n",
    "var = np.std(y)**2\n",
    "def r_squared(mse, var):\n",
    "    return (1-mse/var)\n",
    "\n",
    "for i in range(len(mse)):\n",
    "    print(r_squared(mse[i], var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test accuracy: {score[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune model\n",
    "To make the model become more accurate by implementing:\n",
    "* Fine tune some hyperparameter of number of hidden layers `n_hidden`, numeber of neurons `n_neurons`\n",
    "* Changing the regularization L1 and L2 to reduce the loss and not become overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "    'n_hidden': [2,4,6],\n",
    "    'n_neurons': [32,64],\n",
    "    'a': [0.0002, 0.001],\n",
    "    'b': [0.0002, 0.001]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a model and choose the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cv = GridSearchCV(keras_reg, param_distribs, cv = 3)\n",
    "search_cv.fit(partial_X_train, partial_y_train, epochs=60,validation_data=(x_val,y_val), \\\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=30)])\n",
    "search_cv.best_params_\n",
    "search_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the best parameters after hypertuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = search_cv.best_estimator_.model\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluating the model and making predictions after hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate(X_train, y_train)\n",
    "best_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate(np.array(X_test),np.array(y_test))\n",
    "best_model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
